{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElVAMcAy8M-I",
        "outputId": "2a2662ac-6f53-4f47-eb7c-0f7ff19ba7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize , word_tokenize\n",
        "from natsort import natsorted\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjczz-8E8c9G",
        "outputId": "0da92b7a-84e8-4ffd-fd3c-daa3da717472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUJV77TT8eeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7868374e-6d76-4837-af9f-40eb24f358fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omu4hQ3t8vo-"
      },
      "outputs": [],
      "source": [
        "# Convert the string into lowercase to avoid case sensitive cases \n",
        "def convert_lowercase(data):\n",
        "    return data.lower();\n",
        "\n",
        "#separating words, keywords, phrases, symbols, and other items from a sequence of strings\n",
        "def word_tokenization(data):\n",
        "   word_tokens = word_tokenize(data)\n",
        "   return word_tokens\n",
        "\n",
        "#removing stop words ( common words in english that can be avoided or do not play a significant role in any sentence )\n",
        "def remove_stopwords(data):\n",
        "   stop_words = set(stopwords.words('english'))\n",
        "   result = []\n",
        "   for w in data:\n",
        "      if w not in stop_words:\n",
        "          result.append(w)\n",
        "   return result\n",
        "\n",
        "#removing punctuation marks from string\n",
        "def remove_punctuationmarks(data):\n",
        "    symbols = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    # symbols='''!()-[]{};:'\"\\,<>./?@#$%^&*_~=+'''\n",
        "    for current in data:\n",
        "      if current in symbols:\n",
        "        data = data.replace(current, \" \")\n",
        "    return data\n",
        "\n",
        "#remove blank space from string\n",
        "def remove_blankspace(data):\n",
        "  data = re.sub(' +', ' ', data)\n",
        "  return data\n",
        "\n",
        "# stemming to convert the given word into root word\n",
        "def stemming(data):     \n",
        "    stemmer= PorterStemmer()\n",
        "    new_data = []\n",
        "    for i in data:\n",
        "        new_data.append(stemmer.stem(i))\n",
        "    return new_data\n",
        "\n",
        "# main calling function to call all the above pre processing steps\n",
        "def preprocess(data): \n",
        "     data = remove_blankspace(data)\n",
        "     data = remove_punctuationmarks(data)\n",
        "     data = convert_lowercase(data)\n",
        "     data = word_tokenization(data)\n",
        "     data = remove_stopwords(data)\n",
        "    #  data=stemming(data)\n",
        "     return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = 'lion cat tiger'\n",
        "input_tokens = preprocess(data)\n",
        "l1 = len(input_tokens)\n",
        "input_tokens"
      ],
      "metadata": {
        "id": "HrCv5YgULWaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b67e875-fe37-4bc7-9670-5dc526151e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lion', 'cat', 'tiger']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docMappingList = {}\n",
        "doc_id = 0\n",
        "\n",
        "path = \"/content/drive/MyDrive/Humor,Hist,Media,Food/\"\n",
        "fileList = os.listdir(path)\n",
        "\n",
        "jc = []\n",
        "for i in fileList:\n",
        "   file = open('/content/drive/MyDrive/Humor,Hist,Media,Food/'+ i, 'r', encoding = \"utf-8\", errors =\"ignore\")\n",
        "   text = file.read()\n",
        "   preprocessed_tokens = preprocess(text)\n",
        "   l2 = len(preprocessed_tokens)\n",
        "   int_size = len(set(input_tokens).intersection(preprocessed_tokens))\n",
        "   union_size = l1 + l2 - int_size\n",
        "   jc.append(int_size/union_size) \n",
        "   docMappingList[doc_id] = \"/content/drive/MyDrive/Humor,Hist,Media,Food/\"+i   \n",
        "   doc_id += 1\n",
        "   \n",
        "jc_np = np.array(jc)\n",
        "max_5 = jc_np.argsort()[-5:][::-1]\n",
        "for x in max_5:\n",
        "  print(docMappingList[x])"
      ],
      "metadata": {
        "id": "yPvMPPnULHuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5a0f70-a60d-4808-bb2f-a6348e664ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Humor,Hist,Media,Food/puzzles.jok\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/answers\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/netnews.10\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/failure.txt\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/bagelope.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iTET_XjADTwQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "IR_A2_Q1(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}