{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElVAMcAy8M-I",
        "outputId": "e811e500-6f57-42fb-83f1-b737d7201c1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize , word_tokenize\n",
        "from natsort import natsorted\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjczz-8E8c9G",
        "outputId": "47175cbe-6621-499f-f688-6fb6f7b0e52f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUJV77TT8eeb",
        "outputId": "a9498b5c-f9e5-442a-ebc7-ff22f0df785f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "omu4hQ3t8vo-"
      },
      "outputs": [],
      "source": [
        "# Convert the string into lowercase to avoid case sensitive cases \n",
        "def convert_lowercase(data):\n",
        "    return data.lower();\n",
        "\n",
        "#separating words, keywords, phrases, symbols, and other items from a sequence of strings\n",
        "def word_tokenization(data):\n",
        "   word_tokens = word_tokenize(data)\n",
        "   return word_tokens\n",
        "\n",
        "#removing stop words ( common words in english that can be avoided or do not play a significant role in any sentence )\n",
        "def remove_stopwords(data):\n",
        "   stop_words = set(stopwords.words('english'))\n",
        "   result = []\n",
        "   for w in data:\n",
        "      if w not in stop_words:\n",
        "          result.append(w)\n",
        "   return result\n",
        "\n",
        "#removing punctuation marks from string\n",
        "def remove_punctuationmarks(data):\n",
        "    symbols = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    # symbols='''!()-[]{};:'\"\\,<>./?@#$%^&*_~=+'''\n",
        "    for current in data:\n",
        "      if current in symbols:\n",
        "        data = data.replace(current, \" \")\n",
        "    return data\n",
        "\n",
        "#remove blank space from string\n",
        "def remove_blankspace(data):\n",
        "  data = re.sub(' +', ' ', data)\n",
        "  return data\n",
        "\n",
        "# stemming to convert the given word into root word\n",
        "def stemming(data):     \n",
        "    stemmer= PorterStemmer()\n",
        "    new_data = []\n",
        "    for i in data:\n",
        "        new_data.append(stemmer.stem(i))\n",
        "    return new_data\n",
        "\n",
        "# main calling function to call all the above pre processing steps\n",
        "def preprocess(data): \n",
        "     data = remove_blankspace(data)\n",
        "     data = remove_punctuationmarks(data)\n",
        "     data = convert_lowercase(data)\n",
        "     data = word_tokenization(data)\n",
        "     data = remove_stopwords(data)\n",
        "    #  data=stemming(data)\n",
        "     return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "yPvMPPnULHuW"
      },
      "outputs": [],
      "source": [
        "docMappingList = {}\n",
        "d_docid = 0\n",
        "d_tokenset = set()\n",
        "tokenMapping = {}\n",
        "revtokenMapping = {}\n",
        "d_tid = 0\n",
        "\n",
        "path = \"/content/drive/MyDrive/Humor,Hist,Media,Food/\"\n",
        "fileList = os.listdir(path)\n",
        "\n",
        "for i in fileList:\n",
        "    file = open('/content/drive/MyDrive/Humor,Hist,Media,Food/'+i, 'r', encoding = \"utf-8\", errors =\"ignore\")\n",
        "    text = file.read()\n",
        "    preprocessed_tokens = preprocess(text)\n",
        "    for x in preprocessed_tokens:\n",
        "        if x not in d_tokenset :\n",
        "          d_tokenset.add(x)\n",
        "          tokenMapping[d_tid] = x\n",
        "          revtokenMapping[x] = d_tid\n",
        "          d_tid += 1 \n",
        "    docMappingList[d_docid] = \"/content/drive/MyDrive/Humor,Hist,Media,Food/\" + i \n",
        "    d_docid += 1\n",
        "\n",
        "d_tokenset = list(d_tokenset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bty-ehv8Cfs4"
      },
      "source": [
        "Generating TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "AuacxNM7Ce0R"
      },
      "outputs": [],
      "source": [
        "def gen_tfidf(tf, idf):\n",
        "  rows = len(tf) \n",
        "  tf_idf = [[0 for i in range(d_tid)] for j in range(rows)]\n",
        "  for i in range(rows):\n",
        "    for j in range(d_tid):\n",
        "      tf_idf[i][j] = tf[i][j] * idf[j] \n",
        "  return tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "L5rTchf9RwGX"
      },
      "outputs": [],
      "source": [
        "d_tf = [[0 for i in range(d_tid)] for j in range(d_docid)]\n",
        "r = 0\n",
        "for i in fileList :\n",
        "      file = open('/content/drive/MyDrive/Humor,Hist,Media,Food/'+i, 'r', encoding = \"utf-8\", errors =\"ignore\")\n",
        "      text = file.read()\n",
        "      preprocessed_tokens = preprocess(text)\n",
        "      for x in preprocessed_tokens:\n",
        "          c = revtokenMapping[x]\n",
        "          d_tf[r][c] = d_tf[r][c]+1\n",
        "      r += 1\n",
        "    \n",
        "d_idf = []\n",
        "for i in range(d_tid):\n",
        "  c = 0\n",
        "  for j in range(d_docid):\n",
        "    if d_tf[j][i] > 0:\n",
        "      c += 1\n",
        "  d_idf.append(math.log(d_docid/(c+1)))\n",
        "\n",
        "d_tfidf = gen_tfidf(d_tf, d_idf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxBdBUrKQerJ"
      },
      "source": [
        "Query Vector TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUkNF6DkQdNT",
        "outputId": "1d60eaae-7ceb-4daa-b0e6-5da23546989a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sentence: rinaldo help study\n"
          ]
        }
      ],
      "source": [
        "ip_tokens= []\n",
        "str=input(\"Input sentence: \")\n",
        "ip_tokens = preprocess(str)\n",
        "\n",
        "ip_tf = [[0 for i in range(d_tid)] for j in range(1)]\n",
        "ip_tfidf = [[0 for i in range(d_tid)] for j in range(1)]\n",
        "\n",
        "r = 0\n",
        "for i in ip_tokens :\n",
        "    c = revtokenMapping[i]\n",
        "    ip_tf[0][c] = ip_tf[0][c]+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "vDK5-Eh28gJL"
      },
      "outputs": [],
      "source": [
        "def gen_cos_sim(ip_tf, d_tf, choice):\n",
        "\n",
        "  # BINARY FREQUENCY \n",
        "  if choice==1 :\n",
        "    for i in range(d_docid):\n",
        "      for j in range(d_tid):\n",
        "        d_tf[i][j] = 0 if d_tf[i][j]==0 else 1 \n",
        "    for i in range(d_tid):\n",
        "      ip_tf[0][i] = 0 if ip_tf[0][i]==0 else 1 \n",
        "\n",
        "  # TERM FREQUENCY\n",
        "  elif choice==3 :\n",
        "    for i in range(d_docid):\n",
        "      sum_list = sum(d_tf[i]) \n",
        "      for j in range(d_tid):\n",
        "        d_tf[i][j]  = d_tf[i][j]/sum_list\n",
        "    sum_list = sum(ip_tf[0])\n",
        "    for i in range(d_tid):\n",
        "      ip_tf[0][i] = ip_tf[0][i]/sum_list\n",
        "\n",
        "  # LOG NORMALIZATION\n",
        "  elif choice==4 :\n",
        "    for i in range(d_docid):\n",
        "      for j in range(d_tid):\n",
        "        d_tf[i][j]  = math.log2(1+d_tf[i][j])\n",
        "    for i in range(d_tid):\n",
        "      ip_tf[0][i] = math.log2(1+ip_tf[0][i])\n",
        "   \n",
        "  # DOUBLE NORMALIZATION \n",
        "  elif choice==5 :\n",
        "    for i in range(d_docid):\n",
        "      max_num = max(d_tf[i])\n",
        "      for j in range(d_tid):\n",
        "        d_tf[i][j]  = 0.5 + ( 0.5*( d_tf[i][j] / max_num ) )\n",
        "    max_num = max(ip_tf[0])\n",
        "    for i in range(d_tid):\n",
        "      ip_tf[0][i] = 0.5 + ( 0.5*( ip_tf[0][i] / max_num ) )\n",
        "\n",
        "  #  Calculating IDF for input and document using updated frequency matrix \n",
        "  for i in range(d_docid):\n",
        "    for j in range(d_tid):\n",
        "      d_tfidf[i][j] = d_tf[i][j] * d_idf[j] \n",
        "  \n",
        "  for i in range(d_tid):\n",
        "    ip_tfidf[0][i] = ip_tf[0][i] * d_idf[i]\n",
        "     \n",
        "  # Finding cosine similarity between tf-idf of query and documents one by one \n",
        "  COS = []\n",
        "  for i in range(d_docid):\n",
        "    cs = dot(ip_tfidf[0], d_tfidf[i] )/(norm(ip_tfidf[0])*norm(d_tfidf[i]))\n",
        "    COS.append(cs)\n",
        "\n",
        "  # Finding indexes of 5 maximum cosine similarity \n",
        "  max_5 = np.array(COS).argsort()[-5:][::-1]\n",
        "  for i in range(5):\n",
        "    print(docMappingList[max_5[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BINARY FREQUENCY"
      ],
      "metadata": {
        "id": "eMtuz2NKpvE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjdqoDZ28hi5",
        "outputId": "93b47866-90c4-42d7-abc4-0943ae82c8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldos.txt\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldo.jok\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldos.law\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/resrch_phrase\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/resrch_p.hra\n"
          ]
        }
      ],
      "source": [
        "gen_cos_sim(ip_tf, d_tf, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAW COUNT"
      ],
      "metadata": {
        "id": "tWKplWmgpx6p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaDkgxJlwDrc",
        "outputId": "e18ce0db-a886-49d2-ee4b-908093e58bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldos.txt\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldo.jok\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldos.law\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/resrch_phrase\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/resrch_p.hra\n"
          ]
        }
      ],
      "source": [
        "gen_cos_sim(ip_tf, d_tf,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TERM FREQUENCY"
      ],
      "metadata": {
        "id": "iyxgz2OYp1Wj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkV2YcLn9F93",
        "outputId": "feed5913-1146-4b2e-a7d9-6aedad406203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldos.txt\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldo.jok\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldos.law\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/resrch_phrase\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/resrch_p.hra\n"
          ]
        }
      ],
      "source": [
        "gen_cos_sim(ip_tf, d_tf,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOG NORMALIZATION"
      ],
      "metadata": {
        "id": "XBGZ_iTop5zy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci0VD6_bCR5u",
        "outputId": "29309e54-3567-4701-abe6-eba78cd8d660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldos.txt\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldo.jok\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/rinaldos.law\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/resrch_phrase\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/resrch_p.hra\n"
          ]
        }
      ],
      "source": [
        "gen_cos_sim(ip_tf, d_tf,4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DOUBLE NORMALIZATION"
      ],
      "metadata": {
        "id": "HjQ-uLfQp9hW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mwm8DeACSVi",
        "outputId": "759810c2-033b-4481-dbbb-babe48669361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Humor,Hist,Media,Food/boston.geog\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/flowchrt\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/justify\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/flowchrt.txt\n",
            "/content/drive/MyDrive/Humor,Hist,Media,Food/banana05.brd\n"
          ]
        }
      ],
      "source": [
        "gen_cos_sim(ip_tf, d_tf,5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IR_A2_Q1(2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}